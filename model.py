 #-*- coding: utf-8 -*-
"""FlightFare.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1D6ID9_HmYFYJ6lLneMx-aSG1Ud_ZObsZ
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

sns.set()

train_data = pd.read_excel(r"Data_Train.xlsx")

pd.set_option('display.max_columns',None) # to display all the columns

train_data.head()

train_data.info()

train_data["Duration"].value_counts() # count the number of unique values

train_data.shape

train_data.dropna(inplace=True)

train_data.isnull().sum()

"""**EDA**"""

# frist converting the date time format into 2 features day of journey and date of journey
train_data["Journey_day"] = pd.to_datetime(train_data.Date_of_Journey, format="%d/%m/%Y").dt.day

train_data["Journey_month"]  = pd.to_datetime(train_data.Date_of_Journey, format="%d/%m/%Y").dt.month

train_data.head()

# date of journey is not required now
train_data.drop(["Date_of_Journey"], axis=1, inplace=True)

# do the same thing for departure time
train_data["Dep_hour"] = pd.to_datetime(train_data["Dep_Time"]).dt.hour
train_data["Dep_min"] = pd.to_datetime(train_data["Dep_Time"]).dt.minute
train_data.drop(["Dep_Time"], axis=1,inplace=True)

train_data.head()

# do the same thing with arrival time
train_data["Arr_hour"] = pd.to_datetime(train_data["Arrival_Time"]).dt.hour
train_data["Arr_min"] = pd.to_datetime(train_data["Arrival_Time"]).dt.minute
train_data.drop(["Arrival_Time"], axis=1,inplace=True)

train_data.head()

# extracting the total duration into separate columns of hours and min
duration = list(train_data["Duration"])

for i in range(len(duration)):
  if len(duration[i].split()) !=2: # contains only hours and min
    if "h" in duration[i]:
      duration[i] = duration[i].strip() + " 0m"
    else:
      duration[i] = "0h "+ duration[i]

duration_hours = []
duration_mins = []
for i in range(len(duration)):
  duration_hours.append(int(duration[i].split(sep="h")[0]))
  duration_mins.append(int(duration[i].split(sep="m")[0].split()[-1]))

train_data["Duration_hours"] = duration_hours
train_data["Duration_mins"] = duration_mins

train_data.head()

train_data.drop(["Duration"], axis=1,inplace=True)

"""**Handling categorical Data**"""

# use catplot to visualise
# An airline is Nominal Categorical Data we will perform OneHotEncoding
Airline = train_data[["Airline"]]
print(Airline)
Airline = pd.get_dummies(Airline, drop_first=True) # droping the first column
Airline.head()

train_data["Source"].value_counts() #count each type

sns.catplot(y="Price", x="Source", data = train_data.sort_values("Price"))
plt.show()

# Source is also a nominal cat data then use one hot encoding
Source = train_data[["Source"]]
Source = pd.get_dummies(Source,drop_first=True)
Source.head()

train_data["Destination"].value_counts() #count each type
sns.catplot(y="Price", x="Destination", data = train_data.sort_values("Price"))
plt.show()
Destination = train_data[["Destination"]]
Destination = pd.get_dummies(Destination,drop_first=True)
Destination.head()

train_data["Route"].value_counts()

# the route and stops are highly correlated hence we will drop the route
# however we do not need to have additional info because 80% of it is no_info hence drop
train_data.drop(["Route", "Additional_Info"],axis=1,inplace=True)

train_data["Total_Stops"].value_counts()

# here the stops data is ordinal hence we are using lable encoding for this
train_data.replace({"non-stop":0, "1 stop":1, "2 stops":2, "3 stops":3, "4 stops":4}, inplace=True)

train_data.head()

# Now concatenate all the features encoded
data_train = pd.concat([train_data,Airline,Source,Destination], axis=1)

data_train.head()

data_train.drop(["Airline","Source","Destination"], axis=1, inplace=True)

data_train.head()

"""VVVVVIMP: We need to preprocess the train and test data separately to avoid data leakage
for the model to work properly

**Test Data**
"""

test_data = pd.read_excel(r"Test_set.xlsx")

test_data.head()

test_data.info()
test_data["Duration"].value_counts()
test_data.dropna(inplace=True)
test_data.isnull().sum()
test_data["Journey_day"] = pd.to_datetime(test_data.Date_of_Journey, format="%d/%m/%Y").dt.day
test_data["Journey_month"]  = pd.to_datetime(test_data.Date_of_Journey, format="%d/%m/%Y").dt.month
test_data.drop(["Date_of_Journey"], axis=1, inplace=True)
test_data["Dep_hour"] = pd.to_datetime(test_data["Dep_Time"]).dt.hour
test_data["Dep_min"] = pd.to_datetime(test_data["Dep_Time"]).dt.minute
#test_data.head()
test_data["Arr_hour"] = pd.to_datetime(test_data["Arrival_Time"]).dt.hour
test_data["Arr_min"] = pd.to_datetime(test_data["Arrival_Time"]).dt.minute
test_data.drop(["Arrival_Time"], axis=1,inplace=True)
duration = list(test_data["Duration"])

for i in range(len(duration)):
  if len(duration[i].split()) !=2: # contains only hours and min
    if "h" in duration[i]:
      duration[i] = duration[i].strip() + " 0m"
    else:
      duration[i] = "0h "+ duration[i]

duration_hours = []
duration_mins = []
for i in range(len(duration)):
  duration_hours.append(int(duration[i].split(sep="h")[0]))
  duration_mins.append(int(duration[i].split(sep="m")[0].split()[-1]))
test_data["Duration_hours"] = duration_hours
test_data["Duration_mins"] = duration_mins
test_data.drop(["Duration"], axis=1,inplace=True)
Airline = test_data[["Airline"]]
Airline = pd.get_dummies(Airline, drop_first=True) # droping the first column
#Airline.head()
#test_data["Source"].value_counts() #count each type
Source = test_data[["Source"]]
Source = pd.get_dummies(Source,drop_first=True)
#Source.head()
#test_data["Destination"].value_counts() #count each type
#sns.catplot(y="Price", x="Destination", data = test_data.sort_values("Price"))
#plt.show()
Destination = test_data[["Destination"]]
Destination = pd.get_dummies(Destination,drop_first=True)
#Destination.head()
#test_data["Route"].value_counts()
test_data.drop(["Route", "Additional_Info"],axis=1,inplace=True)
#test_data["Total_Stops"].value_counts()
test_data.replace({"non-stop":0, "1 stop":1, "2 stops":2, "3 stops":3, "4 stops":4}, inplace=True)
data_test = pd.concat([test_data,Airline,Source,Destination], axis=1)
data_test.drop(["Airline","Source","Destination"], axis=1, inplace=True)

data_test.head()

"""**FEATURES SELECTION**"""

#heatmap
#extra tree regressor
X = data_train.loc[:,data_train.columns != 'Price']
X.head()

Y = data_train.iloc[:,1]
Y.head()

# HEATMAP

plt.figure(figsize=(18,18))
sns.heatmap(data_train.corr(),annot=True, cmap="RdYlGn")
plt.show()

"""**Fitting Model Using Random Forest**"""

from sklearn.model_selection import train_test_split
X_train,X_test,y_train,y_test = train_test_split(X,Y,test_size = 0.2, random_state=1)

from sklearn.ensemble import RandomForestRegressor
reg_rf = RandomForestRegressor()
reg_rf.fit(X_train, y_train)

y_pred = reg_rf.predict(X_test)

reg_rf.score(X_train, y_train)

reg_rf.score(X_test,y_test)

sns.distplot(y_test-y_pred)
plt.show()
#gaussian distribution

plt.scatter(y_test, y_pred, alpha=0.5)
plt.xlabel("y_test")
plt.ylabel("y_pred")
plt.show()

from sklearn import metrics
print('MAE:', metrics.mean_absolute_error(y_test, y_pred))
print('MSE:', metrics.mean_squared_error(y_test, y_pred))
print('RMSE:', np.sqrt(metrics.mean_squared_error(y_test, y_pred)))

metrics.r2_score(y_test, y_pred)

"""**HYPERPARAMETER TUNING**"""

from sklearn.model_selection import RandomizedSearchCV

# randomized search cv

# Number of trees in random forest
n_estimators = [int(x) for x in np.linspace(start=100, stop=1200, num=12)]
#Number of features to consider at every split
max_features = ['auto', 'sqrt']
#Maximum number of levels in tree
max_depth = [int(x) for x in np.linspace(5,30,num=6)]
#minimum number of samples required to split a node
min_samples_split = [2,5,10,15,100]
#min number of samples req at each leaf node
min_samples_leaf = [1,2,5,10]

# create the random grid

random_grid = {
    'n_estimators': n_estimators,
    'max_features': max_features,
    'max_depth': max_depth,
    'min_samples_split': min_samples_split,
    'min_samples_leaf': min_samples_leaf
}

# random search of parameters using 5 fold cross validation,
# search across 100 diffirent combinations
rf_random = RandomizedSearchCV(estimator=reg_rf, param_distributions=random_grid, scoring='neg_mean_squared_error', n_iter=10 ,cv=5, verbose=2, random_state=42, n_jobs=1)

rf_random.fit(X_train,y_train)

rf_random.best_params_

prediction = rf_random.predict(X_test)

plt.figure(figsize=(8,8))
sns.distplot(y_test-prediction)
plt.show()

plt.figure(figsize=(8,8))
plt.scatter(y_test, prediction, alpha=0.5)
plt.xlabel("y_test")
plt.ylabel("y_pred")
plt.show()

print('MAE:', metrics.mean_absolute_error(y_test, prediction))
print('MSE:', metrics.mean_squared_error(y_test, prediction))
print('RMSE:', np.sqrt(metrics.mean_squared_error(y_test, prediction)))

"""make a file"""

import pickle
file = open('FlightFare.pkl','wb')
pickle.dump(rf_random,file)

model = open('FlightFare.pkl','rb')
forest = pickle.load(model)

y_prediction = forest.predict(X_test)

metrics.r2_score(y_test, y_prediction)
